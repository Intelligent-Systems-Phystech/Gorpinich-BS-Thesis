\documentclass[12pt, aspectratio=169]{beamer}
\beamertemplatenavigationsymbolsempty
\usecolortheme{beaver}
\setbeamertemplate{blocks}[rounded=true, shadow=true]
\setbeamersize{text margin left=5pt,text margin right=5pt}
\setbeamertemplate{footline}[page number]
\input{slides/math_symbols_slides}
%
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amssymb,amsfonts,amsmath,mathtext}
\usepackage{subfig}
\usepackage[all]{xy} % xy package for diagrams
\usepackage{array}
\usepackage{multicol}% many columns in slide
\usepackage{hyperref}% urls
\usepackage{hhline}%tables
\usepackage{tabularx}
\usepackage{algorithmic}
\usepackage{algorithm}
\usepackage[style=numeric,sorting=none]{biblatex}
% Your figures are here:
%\graphicspath{ {fig/} {../fig/} }

%----------------------------------------------------------------------------------------------------------
\addbibresource{bibliography_eng.bib}
\nocite{*}
\title[\hbox to 56mm{Optimizing regularization path in knowledge distillation}]{Optimizing regularization path in knowledge distillation}
\author[M.~Gorpinich, O.\,Yu.~Bakhteev, V.\,V.~Strijov]{M.~Gorpinich, O.\,Yu.~Bakhteev, V.\,V.~Strijov}
% \begingroup
% \fontsize{8pt}{10pt}\selectfont
\institute{\fontsize{11}{14}\selectfont Moscow Institute of Physics and Technology}
% \endgroup
\date{\footnotesize
% \par\smallskip\emph{Курс:} Автоматизация научных исследований\par (практика, В.\,В.~Стрижов)/Группа 874
% \par\smallskip\emph{Expert:} V.\,V.~Strijov
% \par\smallskip\emph{:} О.\,Ю.~Bakhteev
% \par\smallskip M.~Gorpinich, V.\,V.~Strijov, O.\,Yu.~Bakhteev
\par\bigskip\small 2021}
%----------------------------------------------------------------------------------------------------------
\begin{document}
%----------------------------------------------------------------------------------------------------------
\begin{frame}
\thispagestyle{empty}
\maketitle
\end{frame}
%-----------------------------------------------------------------------------------------------------

\begin{frame}{Distillation knowledge}
\begin{block}{Purpose}
    To propose a metaparameter optimization method in the knowledge distillation task.
\end{block}
\begin{block}{Problem}
    Metaparameter selection is a computationally expensive problem.
\end{block}
% \begin{block}{Метод исследования} 
\begin{block}{Solution}
    Formulate bi-level optimization problem. We solve the problem by gradient-based methods. To accelerate the computationally expensive metaparameter optimization we predict with linear models.
\end{block}
\end{frame}

\begin{frame}{Optimization procedure}
    \begin{figure}
    \caption*{}
    \vspace{-1.26 cm}
    \includegraphics[width=0.37\textwidth]{trajectory_.pdf}
\end{figure}

A principle idea of the proposed method. The metaparameters control the final loss of the model. Instead of optimize the metaparameters straightforwardly we analyze the optimization trajectory and predict it using linear model.

\end{frame}

\begin{frame}{Problem description}

    {\color{red}Metaparameters} \boldsymbol{\lambda} are the parameters of knowledge distillation optimization problem. Namely, the coefficients before terms in loss function and the temperature: 
    $$\boldsymbol{\lambda} = [\lambda_1, T].$$
    The temperature is a factor of logits of models in softmax function.
    
    \vspace{0.2 cm}
    
    {\color{red}Knowledge distillation} is the model parameter optimization problem. It takes:
    \begin{enumerate}
        \item information of the initial dataset;
        \item information of the cumbersome model.
    \end{enumerate}
    
    \vspace{0.2 cm}
    
    The {\color{red}teacher model} has a more complex structure. It is trained using the initial dataset. The {\color{red}student model} has a simpler structure. We optimize it by transferring the knowledge of the cumbersome model to it.
    
\end{frame}
%-----------------------------------------------------------------------------------------------------

% \begin{frame}{Model parameter optimization using knowledge distillation}

% Рассматривается задача дистилляции модели. Будем корректировать траекторию оптимизации на основе двухуровневой задачи оптимизации:

% $$ \hat{\bh} = \argmax\limits_{\bh \in \bbR^2} \sum\limits_{(\bx, y) \in \fD_\text{val}}\sum\limits_{k=1}^{K}y^k\log \bg(\bx, \bw)|_{T=1} $$
% $$ \hat{\bw} = \argmax\limits_{\bw \in \bbR^s} (1-\lambda)\sum\limits_{(\bx, y) \in \fD_\text{train}}\sum\limits_{k=1}^{K}y^k\log \bg(\bx, \bw)|_{T=1} + \lambda\sum\limits_{(\bx, y) \in \fD_\text{train}}\sum\limits_{k=1}^{K}\bff(\bx)|_{T=T_0}\log \bg(\bx, \bw)|_{T=T_0} $$

% где $\bh = [\lambda, T_0]$ --- параметры дистилляционного слагаемого.

% \begin{columns}[c]
% \column{0.5\textwidth}
% Рассматривается задача дистилляции модели. Будем корректировать траекторию оптимизации на основе двухуровневой задачи оптимизации:

% $$ \hat{\bh} = \argmax\limits_{\bh \in \bbR^2} \cL_\text{val}(\hat{\bw}, \bh) $$
% $$ \hat{\bw} = \argmin\limits_{\bw \in \bbR^s} \cL_\text{train}(\bw, \bh) $$

% где $\bh$ --- параметры дистилляционного слагаемого.
% \column{0.5\textwidth}
% $$\cL_\text{train}(\bw, \bh) = -\sum\limits_{(\bx, y) \in \fD_\text{train}}\sum\limits_{k=1}^{K}y^k\log \bg(\bx, \bw)|_{T=1} $$$$- \lambda\sum\limits_{(\bx, y) \in \fD_\text{train}}\sum\limits_{k=1}^{K}\bff(\bx)|_{T=T_0}\log \bg(\bx, \bw)|_{T=T_0}$$

% $$\cL_\text{val}(\bw, \bh) = \sum\limits_{(\bx, y) \in \fD_\text{val}}\sum\limits_{k=1}^{K}y^k\log \bg(\bx, \bw)|_{T=1}$$
% \end{columns}

% Оптимизация гиперпараметров:

% $$\bh^\prime = \bh - \gamma_{\bh}\nabla_{\bh}\cL_\text{val}(\bw - \gamma\nabla\cL_\text{train}(\bw, \bh), \bh)$$

% Назовем {\color{red}дистилляцией знаний} задачу оптимизации параметров модели, при которой учитывается информация, содержащаяся в выборке и в сторонней модели (модели-учителе).

% \centering
% \begin{columns}[c]
% \column{0.5\textwidth}
% \centering
% \includegraphics[width=\textwidth]{slides_eng/scatter_temp_beta2_eng.pdf}
% \hspace{-2 cm}
% \column{0.5\textwidth}
% $T$ is a temperature, $\lambda_1$ is a  fraction of the likelihood of the initial dataset in the loss function

% \end{columns}
% \end{frame}



\begin{frame}{Main references}
    % \bibliographystyle{slides_bibstyle.bst}
    % \bibliography{slides_bibliography.bib}
    \printbibliography
\end{frame}

%----------------------------------------------------------------------------------------------------------
\begin{frame}{Distillation problem statement}
\begin{block}{There is given a dataset}
\vspace{-0.5 cm}

$$\fD = \{(\bx_i, y_i)\}_{i=1}^{m},\; \bx_i \in \bbR^n,\qquad y_i \in \bbY = \{1, \dots, K\},\qquad \fD = \fD_\text{train} \sqcup \fD_\text{val}.$$
\end{block}

\vspace{-0.3 cm}
$\bff$ is the fixed teacher model, $\bg$ is the student model.
\vspace{1 cm}

\textbf{Definition 1.}
Let function~$D: \bbR^s \to \bbR_{+}$ defines the distance between the student model~$\bg$ and the teacher model~$\bff$. $D$-distillation of a student model is a student model parameter optimization problem that minimizes the function~$D$.
\end{frame}

\begin{frame}{Loss functions}
\fontsize{10}{5}\selectfont
\begin{block}{Train (distillation) loss}
\vspace{-0.7 cm}
\fontsize{10}{5}\selectfont
\begin{multline*}
    \cL_\text{train}(\bw, \boldsymbol{\lambda}) = -\lambda_1\sum\limits_{(\bx, y) \in \fD_\text{train}}\underbrace{\sum\limits_{k=1}^{K}y^k\log \frac{e^{\bg(\bx, \bw)_k}}{\sum\limits_{j=1}^{K}e^{\bg(\bx, \bw)_j}}}_{\text{classification term}} - (1 - \lambda_1)\sum\limits_{(\bx, y) \in \fD_\text{train}}\underbrace{\sum\limits_{k=1}^{K}\frac{e^{\bff(\bx)_k/T}}{\sum\limits_{j=1}^{K}e^{\bff(\bx)_j/T}}\log \frac{e^{\bg(\bx, \bw)_k/T}}{\sum\limits_{j=1}^{K}e^{\bg(\bx, \bw)_j/T}}}_{\text{distillation term}},
\end{multline*}
\end{block}
\vspace{-0.5 cm}
\fontsize{10}{5}\selectfont
\begin{block}{Validation loss}
\vspace{-0.5 cm}
\fontsize{10}{5}\selectfont
\begin{equation*} \label{eq:l_val}
     \cL_\text{val}(\bw, \boldsymbol{\lambda}) = - \sum\limits_{(\bx, y) \in \fD_\text{val}}\sum\limits_{k=1}^{K}y^k\log \frac{e^{\bg(\bx, \bw)_k/T_\text{val}}}{\sum\limits_{j=1}^Ke^{\bg(\bx, \bw)_j/T_\text{val}}}
\end{equation*}
\end{block}
\vspace{-0.2 cm}
\fontsize{10}{5}\selectfont
Metaparameter set:
\vspace{-0.2 cm}
\fontsize{10}{5}\selectfont
$$\boldsymbol{\lambda} = [\lambda_1, T]$$
\fontsize{10}{5}\selectfont
Optimization problem:
\vspace{-0.2 cm}
\fontsize{10}{5}\selectfont

\begin{equation}
    \hat{\boldsymbol{\lambda}} = \arg\min\limits_{\boldsymbol{\lambda} \in \bbR^2} \cL_\text{val}(\hat{\bw}, \boldsymbol{\lambda}),
    \label{eq:opt_hyp}
\end{equation}

\begin{equation*}
    \hat{\bw} = \arg\min\limits_{\bw \in \bbR^s} \cL_\text{train}(\bw, \boldsymbol{\lambda}).
\end{equation*}
\end{frame}

\begin{frame}{Special case of D-distillation}
% \fontsize{10}{5}\selectfont
\textbf{Lemma 1.}
When~$\lambda_1 = 0$ we minimize loss function which is a $D$-distillation with $D = D_{KL}\left(\sigma\left(\bff/T\right), \sigma\left(\bg/T\right)\right)$, where $\sigma$ is a softmax function.

When~$\lambda_1 = 0$ we have:
% \vspace{-0.5 cm}
% \fontsize{10}{5}\selectfont
\begin{multline*}
    \cL_\text{train}(\bw, \boldsymbol{\lambda}) = \sum\limits_{(\bx, y) \in \fD_\text{train}}\sum\limits_{k=1}^{K}\frac{e^{\bff(\bx)_k/T}}{\sum\limits_{j=1}^{K}e^{\bff(\bx)_j/T}}\log \frac{e^{\bg(\bx, \bw)_k/T}}{\sum\limits_{j=1}^{K}e^{\bg(\bx, \bw)_j/T}} \\= D_{KL}\left(\sigma(\bff(\bx)/T), \sigma(\bg(\bx, \bw)/T)\right).
\end{multline*}
% \vspace{-0.2 cm}

The function $D_{KL}\left(\sigma\left(\bff/T\right), \sigma\left(\bg/T\right)\right)$ defines the distance between logits of model $\bff$ and model $\bg$. Therefore, the definition of the $D$-distillation is satisfied.
% \vspace{-0.2 cm}
% \fontsize{10}{5}\selectfont
% \begin{multline*}
% \lim_{t \to \infty} D_{KL}(\sigma(\bff(\bx)/T|_{T=T_0})_i, \sigma(\bg(\bx, \bw_t)/T|_{T=T_0})_i)\\=\min_{\bw^\prime_t \in \bbR^s} D_{KL}(\sigma(\bff(\bx)/T|_{T=T_0})_i, \sigma(\bg(\bx, \bw^\prime_t)/T|_{T=T_0})_i)
% \end{multline*}
% \vspace{-0.2 cm}

% where $\sigma$ is a softmax funcion. 
% % \vspace{-0.2 cm}

% We use the following inequality:
% \vspace{-0.2 cm}
% \fontsize{10}{5}\selectfont
% $$D_{KL}(\sigma(\bff(\bx)/T|_{T=T_0})_i, \sigma(\bg(\bx, \bw^\prime_{k+1}))) \le D_{KL}(\sigma(\bff(\bx)/T|_{T=T_0})_i, \sigma(\bg(\bx, \bw^\prime_k)/T|_{T=T_0})_i)$$
% \vspace{-0.2 cm}

% We also have
% % \vspace{-0.2 cm}
% \fontsize{10}{5}\selectfont
% $D_{KL}(\sigma(\bff(\bx)/T|_{T=T_0})_i, \sigma(\bg(\bx, \bw)_T|_{T=T_0})_i) \ge 0.$
% % \vspace{-0.2 cm}

% Then
% \vspace{-0.2 cm}
% \fontsize{10}{5}\selectfont
% $$\lim_{t \to \infty} D_{KL}(\sigma(\bff(\bx)/T|_{T=T_0})_i, \sigma(\bg(\bx, \bw_t)/T|_{T=T_0})_i)=\min_{\bw^\prime_t \in \bbR^s} D_{KL}(\sigma(\bff(\bx)/T|_{T=T_0})_i, \sigma(\bg(\bx, \bw^\prime_t)_T|_{T=T_0})_i)$$
% \vspace{-0.2 cm}
% So $D_{KL}(\sigma(\bff/T|_{T=T_0})_i, \sigma(\bg/T|_{T=T_0})_i)$ is a special case of $D$-distillation.

\end{frame}

\begin{frame}{Gradient-based optimization}

\begin{columns}[c]
\column{0.7\textwidth}
\fontsize{11}{5}\selectfont
\textbf{Definition 2.} \emph{Optimization operator} is an algorithm~$U$ of parameter vector~$\bw^\prime$ selection using the parameters on the next step~$\bw$:
\vspace{-0.2 cm}
\fontsize{10}{5}\selectfont
\begin{equation*}
    \bw^\prime = U(\bw).
\end{equation*}

\fontsize{11}{5}\selectfont
Optimize the parameters~$\bw$ using~$\eta$ optimization steps:
\vspace{-0.2 cm}
\fontsize{10}{5}\selectfont
\begin{equation*}
    \hat{\bw} = U \circ U \circ \dots \circ U(\bw_0, \boldsymbol{\lambda}) = U^\eta(\bw_0, \boldsymbol{\lambda}),
\end{equation*}

\noindent
\fontsize{11}{5}\selectfont
where~$\bw_0$ is an initial value of parameter vector~$\bw$.
\fontsize{11}{5}\selectfont
Redefine the minimization problem using the definition of operator~$U$:
\vspace{-0.2 cm}
$$\hat{\boldsymbol{\lambda}} = \argmax\limits_{\boldsymbol{\lambda} \in \bbR^3} \cL_\text{val}\bigl(U^\eta(\bw_0, \boldsymbol{\lambda})\bigr), \qquad U(\bw, \boldsymbol{\lambda}) = \bw - \gamma\nabla\cL_\text{train}(\bw, \boldsymbol{\lambda}).$$
\fontsize{11}{5}\selectfont
Update metaparameters successively according to the rule:
\vspace{-0.2 cm}
\fontsize{10}{5}\selectfont
\begin{equation}
\boldsymbol{\lambda}^\prime = \boldsymbol{\lambda} - \gamma_{\boldsymbol{\lambda}}\nabla_{\boldsymbol{\lambda}}\cL_\text{val}(U(\bw, \boldsymbol{\lambda}), \boldsymbol{\lambda}) = \boldsymbol{\lambda} - \gamma_{\boldsymbol{\lambda}}\nabla_{\boldsymbol{\lambda}}\cL_\text{val}(\bw - \gamma\nabla\cL_\text{train}(\bw, \boldsymbol{\lambda}), \boldsymbol{\lambda}).
\label{eq:hyp_alg}
\end{equation}

\column{0.25\textwidth}
\fontsize{12}{10}\selectfont
{\color{red}Hypothesis}: gradient optimization regularization path can be approximated with locally linear model:
% \vspace{-0.2 cm}
\fontsize{12}{10}\selectfont
$$\boldsymbol{\lambda}^\prime = 
     \boldsymbol{\lambda} + \bc^{\top}\begin{pmatrix}z\\1\end{pmatrix},$$
% \vspace{-0.2 cm}
\fontsize{12}{10}\selectfont
where $z$ is a remainder of iteration number divided by the period of linear model training, $\bc$ is a vector of linear model parameters.
\end{columns}
% \begin{equation}
%     \nabla \boldsymbol{\lambda}^\prime = 
%     \nabla \boldsymbol{\lambda} + \bw^{\T}\begin{pmatrix}j\\1\end{pmatrix}
% \end{equation}
% \begin{equation}
%     \boldsymbol{\lambda}^{\prime\prime} = \begin{pmatrix}1\\1\\10\end{pmatrix}^{\T} \sigma(\boldsymbol{\lambda}^\prime)
% \end{equation}

\end{frame}

\begin{frame}{Resulting algorithm}
    \begin{figure}
\begin{algorithm}[H]
\caption{Metaparameter optimization}
 \begin{algorithmic}[1]

 %\renewcommand{\algorithmicrequire}{\mathbf{Input:}}
 %\renewcommand{\algorithmicensure}{\mathbf{Output:}}
 \REQUIRE number $e_1$ of iterations to use gradient-based optimization 
 \REQUIRE number $e_2$ of iterations to use linear predictions for metaparameters $\boldsymbol{\lambda}$  
 %\REQUIRE in
 %\ENSURE  out
  \WHILE {not converged}
  \STATE Optimize $\boldsymbol{\lambda}$ and $\mathbf{w}$ for $e_1$ iterations solving a bi-level optimization problem
  \STATE $\textbf{traj} = $trajectory of $(\nabla \boldsymbol{\lambda})$ changes during optimization;
  \STATE Set $\mathbf{z} = [1,\dots,e_1]^\mathsf{T}$
  \STATE Optimize $\mathbf{c}$ using least square method: 
  $$\hat{\mathbf{c}} = \argmin_{\mathbf{c} \in \mathbb{R}^2} ||\textbf{traj} - \mathbf{z}\cdot c_1 + c_2||_2^2$$
  \STATE Optimize $\mathbf{w}$ and predict $\boldsymbol{\lambda}$ for $e_2$ iterations using linear model with parameters $\mathbf{c}$.
  \ENDWHILE

 \end{algorithmic}
 \end{algorithm}
 \caption{An algorithm for the proposed method.}
 \label{algo}

 \end{figure}
\end{frame}

\begin{frame}{Correctness of the approximation by a linear model}

\textbf{Theorem 1.} If function $\mathcal{L}_{\textnormal{train}}(\bw, \boldsymbol{\lambda})$ is smooth and convex and its Hessian $\mathbf{H} = \nabla_{\bw}^2 \mathcal{L}_\textnormal{train}$ is invertible and can be well approximated by identity, $\mathbf{H} \approx \mathbf{I},$ then greedy algorithm~\eqref{eq:hyp_alg} finds optimum solution of the bi-level problem \eqref{eq:opt_hyp}. If there is a domain $\cD \in \bbR^2$ in metaparameter space where the gradient of metaparameters can be well approximated by a constant, then the optimization is linear w.r.t. the metaparameters.

\end{frame}

% \begin{frame}{Градиентные методы оптимизации}


% \end{frame}
%----------------------------------------------------------------------------------------------------------

%----------------------------------------------------------------------------------------------------------

\begin{frame}{Experiment setup}
    
% Test the hypothesis on the approximation of the regularization path by a linear model.
    
\begin{block}{Datasets}
    Synthetic dataset, CIFAR-10 (whole dataset and reduced training subset), Fashion-MNIST
\end{block}
    
\begin{block}{Optimization methods}
\fontsize{11}{5}\selectfont
\begin{enumerate}[{1)}]
    \item optimization without distillation;
    \item optimization with randomly initialized metaparameter values. The metaparameters were sampled from uniform distribution
    \vspace{-0.3 cm}
    $$\lambda_1 \sim \mathcal{U}(0;1), \quad T \sim \mathcal{U}(0.1, 10).$$
    \vspace{-0.5 cm}
    \item optimization with ``naive'' metaparameter assignment: setting
    \vspace{-0.3 cm}
    $$\lambda_1 = 0.5, T = 1;$$
    \vspace{-0.5 cm}
    \item gradient-based optimization;
    \item proposed method with $e_1=e_2=10$.
\end{enumerate}
\end{block}
    
\fontsize{11}{5}\selectfont
The external criterion:
\begin{equation*}
    \text{accuracy} = \frac{1}{m}\sum\limits_{i=1}^m [\bg(\bx_i, \bw) = y_i].
\end{equation*}
    
\end{frame}

\begin{frame}{Experiment on the synthetic data}

\begin{block}{The dataset}
% \vspace{-1 cm}
\fontsize{11}{5}\selectfont
$$\fD = \{(\bx_i, y_i)\}_{i=1}^{m},\; x_{ij} \in \cN(0, 1),\qquad j=1, 2, \qquad x_{i3} = [\text{sign}(x_{i1})+\text{sign}(x_{i2})>0],$$
$$y_i = \text{sign}(x_{i1} \cdot x_{i2}+\delta) \in \bbY.$$
The size of student model dataset is significantly smaller then the one of the teacher model.
\vspace{-0.2 cm}
\end{block}
\begin{figure}
    \fontsize{5}{5}\selectfont
    \begin{minipage}[h]{0.3\linewidth}
    \center{
    \includegraphics[width=\linewidth]{ttrain.pdf}\\a)}
    \end{minipage}
    \begin{minipage}[h]{0.3\linewidth}
    \center{
    \includegraphics[width=\linewidth]{train.pdf}\\b)}
    \end{minipage}
    \begin{minipage}[h]{0.3\linewidth}
    \center{
    \includegraphics[width=\linewidth]{test.pdf}\\c)}
    \end{minipage}
    \vspace{-0.2 cm}
    \caption*{\fontsize{8}{5}\selectfont
    Visualization of a) teacher model dataset; b) student model dataset; c) test part}
\end{figure}
\end{frame}

\begin{frame}{Optimization procedure tuning}

    % \begin{figure}
    % \begin{minipage}[h]{0.45\linewidth}
    % \center{
    % \includegraphics[width=\linewidth]{slides_eng/linear_epoch_size_eng.pdf}\\a)}
    % \end{minipage}
    % \begin{minipage}[h]{0.45\linewidth}
    % \center{
    % \includegraphics[width=\linewidth]{slides_eng/linear_train_splines_every_epoch_eng.pdf}\\b)}
    % \end{minipage}
    % \vspace{-0.2 cm}
    % \caption*{\fontsize{10}{5}\selectfont
    % Model accuracy with different a) epoch size; b) rerun periodicity}
    % \end{figure}
\fontsize{8}{5}\selectfont
    \begin{figure}
    \caption*{\fontsize{11}{5}\selectfont
    Model accuracy with $e_1$ and $e_2$ values: a) $e_1 = e_2$; b) variation of $e_2$ with $e_1 =10$.}
    \vspace{-0.2 cm}
    \begin{minipage}[h]{0.45\linewidth}
    \center{
    \includegraphics[width=\linewidth]{synth_mini_epoch_size.pdf}\\(a)}
    \end{minipage}
    \begin{minipage}[h]{0.45\linewidth}
    \center{
    \includegraphics[width=\linewidth]{synth_period.pdf}\\(b)}
    \end{minipage}
    
    
\end{figure}
    
    \fontsize{12}{8}\selectfont
    The best accuracy is obtained with $e_1 = e_2=10$.
    
\end{frame}

% \begin{frame}{Выбор периодичности перезапусков}
% График зависимости точности классификации от номера итерации при различном количестве перезапусков
% \begin{figure}
%     \includegraphics[width=0.55\textwidth]{linear_train_splines_every_epoch_eng.pdf}
% \end{figure}
    

% \end{frame}

% \begin{frame}{Meta-parameter update}
% \fontsize{6}{5}\selectfont
% \begin{figure}
%     \caption*{\fontsize{10}{12}\selectfont
%     Synthetic data, sequence of updated a) $\lambda_1$; b) $\lambda_2$; c) temperature}
%     \vspace{-0.3 cm}
%     \begin{minipage}[h]{0.325\linewidth}
%     \center{
%     \includegraphics[width=\linewidth]{beta1_iter_eng2.pdf}\\a)}
%     \end{minipage}
%     \begin{minipage}[h]{0.325\linewidth}
%     \center{
%     \includegraphics[width=\linewidth]{beta2_iter_eng2.pdf}\\b)}
%     \end{minipage}
%     \vspace{-0.2 cm}
%     \begin{minipage}[h]{0.325\linewidth}
%     \center{
%     \includegraphics[width=\linewidth]{temp_iter_eng2.pdf}\\c)}
%     \end{minipage}
% \end{figure}
% \end{frame}

\begin{frame}{Comparison of optimization approaches}
\begin{figure}
    \caption*{Synthetic data, model accuracy}
    \vspace{-1 cm}
    \includegraphics[width=0.53\textwidth]{synth_accuracy.pdf}
\end{figure}
The results obtained with gradient-based optimization are close to the ones obtained after usage of linear models.
\end{frame}

\begin{frame}{CIFAR-10 dataset}
    The method is evaluated on the whole and reduced ($|\mathfrak{D}_\text{train}|=12800$) CIFAR-10 dataset of~$60000$ color images~$32 \times 32$ in 10 mutually exclusive classes. Each class contains~$6000$ images. Dataset is divided into the train part and test part. Train part contains $5000$ images. Test part contains $1000$ images.
\begin{figure}
    % \vspace{-1 cm}
    \includegraphics[width=0.5\textwidth]{slides_eng/data.pdf}
    % \caption*{CIFAR-10, model accuracy}
\end{figure}
\end{frame}

% \begin{frame}{Basic experiment}
%     \fontsize{6}{5}\selectfont
% \begin{columns}[c]
% \hspace{-0.2 cm}
% \column{0.7\textwidth}
% \begin{figure}
%     \caption*{%\fontsize{10}{12}\selectfont
%     Model accuracy depending on a) $\lambda_1$; b) temperature}
%     \vspace{-0.3 cm}
%     \begin{minipage}[h]{0.45\linewidth}
%     \center{
%     \includegraphics[width=\linewidth]{slides_eng/scatter_beta_acc_eng.png}\\a)}
%     \end{minipage}
%     \hspace{-0.2 cm}
%     \begin{minipage}[h]{0.45\linewidth}
%     \center{
%     \includegraphics[width=\linewidth]{slides_eng/scatter_temp_acc_eng.png}\\b)}
%     \end{minipage}
% \end{figure}
% \hspace{-2 cm}
% \column{0.32\textwidth}
% \hspace{-2 cm}
% \begin{figure}
%     \caption*{%\fontsize{10}{12}\selectfont
%     Model accuracy depending on $\lambda_1$ and temperature}
%     \center{
%     \includegraphics[width=\linewidth]{slides_eng/scatter_temp_beta2_eng.pdf}}
% \end{figure}
% \end{columns}
% \begin{figure}
%     \caption*{\fontsize{10}{12}\selectfont
%     Model accuracy depending on a) $\lambda_1$; b) temperature; c) температуры}
%     \vspace{-0.3 cm}
%     \begin{minipage}[h]{0.325\linewidth}
%     \center{
%     \includegraphics[width=\linewidth]{slides_eng/scatter_beta_acc_eng.png}\\а)}
%     \end{minipage}
%     \begin{minipage}[h]{0.325\linewidth}
%     \center{
%     \includegraphics[width=\linewidth]{slides_eng/scatter_temp_acc_eng.png}\\б)}
%     \end{minipage}
%     \vspace{-0.2 cm}
%     \begin{minipage}[h]{0.325\linewidth}
%     \center{
%     \includegraphics[width=\linewidth]{slides_eng/scatter_temp_beta2_eng.pdf}\\в)}
%     \end{minipage}
% \end{figure}    
% \end{frame}

% \begin{frame}{Meta-parameter update}
% \fontsize{6}{5}\selectfont
% \begin{figure}[!ht]
% \caption*{CIFAR-10, sequence of updated a)~$\lambda_1$; b)~$\lambda_2$; c) temperature}
% \begin{minipage}[h]{0.325\linewidth}
% \center{\includegraphics[width=\linewidth]{slides_eng/beta1_iter19_eng.pdf}\\a)}
% \vspace{-0.3 cm}
% \end{minipage}
% \begin{minipage}[h]{0.325\linewidth}
% \center{\includegraphics[width=\linewidth]{slides_eng/beta2_iter19_eng.pdf}\\b)}
% \end{minipage}
% \vspace{-0.3 cm}
% \begin{minipage}[h]{0.325\linewidth}
% \center{\includegraphics[width=\linewidth]{slides_eng/temp_iter19_eng.pdf}\\c)}
% \end{minipage}
% \end{figure}
% \end{frame}

\begin{frame}{Experiment results on CIFAR-10 dataset}
\begin{figure}
    \caption*{CIFAR-10, model accuracy}
    \vspace{-1 cm}
    \includegraphics[width=0.53\textwidth]{mini_cifar_accuracy.pdf}
\end{figure}
The model accuracy of training using distillation is higher than without it. We obtain the best accuracy if the proposed method is used.
\end{frame}

\begin{frame}{Experiment results}
%     \begin{table}[!ht]
%     \centering
%     \begin{tabular}{|c||p{3 cm}|p{4 cm}|p{4 cm}|}
%     \hline
%     Experiment & Accuracy~without distillation & Accuracy with metaparameter prediction & Accuracy with metaparameter optimization\\
%     \hline
%     Synthetic & 0.7599 & 0.8259 & 0.8299\\
%     CIFAR-10 & 0.5465 & 0.5961 & 0.6006\\
%     \hline
%     \end{tabular}
%     \label{tab:res}
% \end{table}
\begin{table}
\label{table:results}
\footnotesize
\centering
\begin{tabularx}{\textwidth}{|X|X|X|X|X|X}
\cline{1-5}
Method                      & Synthetic dataset & Fashion-MNIST & Reduced CIFAR-10 & CIFAR-10      &  \\ \cline{1-5}
Without distillation        & 0.63 (0.63)             & 0.87  (0.88)        & 0.55     (0.56)        & 0.65 (0.66)         &  \\ \cline{1-5}
Naive metaparameters        & 0.63  (0.63)              & 0.87 (0.88)         & 0.55  (0.56)             & 0.66  (0.67)        &  \\ \cline{1-5}
Random metaparameters       & 0.64   (0.72)           & 0.79   (0.88)       & 0.54 (0.57)             & 0.64 (0.67)        &  \\ \cline{1-5}
Gradient-based optimization & \textbf{0.77} (0.78)    & \textbf{0.88} (0.89) & \textbf{0.57} (0.61)    & \textbf{0.70} (0.72) &  \\ \cline{1-5}
Hyperopt                    & \textbf{0.77} (0.78)                & 0.87 (0.88)         & 0.55  (0.58)           & -             &  \\ \cline{1-5}
Proposed                    & 0.76   (0.78)           & \textbf{0.88} (0.89) & \textbf{0.57}    & \textbf{0.70} (0.72) &  \\ \cline{1-5}
\end{tabularx}
\end{table}

\vspace{0.5 cm}
% The model accuracy obtained after usage of our method is only slightly lower than the one after usage of metaparameter optimization. The proposed method is less computationally expensive comparing to the usage of only gradient optimization.

The  both  the  proposed method and gradient-based method give quite competitive results. The gradient-based methods are preferable because they have similar performance but require fewer  optimization  iterations.

\end{frame}
%----------------------------------------------------------------------------------------------------------
\begin{frame}{Conclusion}
    \begin{itemize}
        \item The usage of gradient-based methods to optimize metaparameters of the distillation task is analyzed. 
        \item The hypothesis about the approximation of regularization path is tested.
        \item The computational experiment showed that the proposed approach to metaparameter optimization can be applied to the distillation task.
        \item The possibility of metaparameter approximation with linear models is confirmed.
        \item The further investigation and the analysis of the quality of metaparameter optimization trajectory approximation with more complex predictive models are planned.
    \end{itemize}
\end{frame}
%----------------------------------------------------------------------------------------------------------
% \end{document} 
% \end{frame}
%-----------------------------------------------------------------------------------------------------


% \end{frame}
%----------------------------------------------------------------------------------------------------------
% \begin{frame}{���������� ������}
% \end{frame}
%----------------------------------------------------------------------------------------------------------
% \begin{frame}{�������}
% \end{frame}
%----------------------------------------------------------------------------------------------------------
% \begin{frame}{�������������� �����������}
% \end{frame}
%----------------------------------------------------------------------------------------------------------
% \begin{frame}{����������}
% \end{frame}
%----------------------------------------------------------------------------------------------------------
\end{document}  